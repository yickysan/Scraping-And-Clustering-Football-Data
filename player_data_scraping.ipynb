{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6cc6446",
   "metadata": {},
   "source": [
    "## Scraping Football Stats Data With Selenium and BeautifulSoup.\n",
    "\n",
    "The goal of this project is to scrape football stats data that will be used for a KMeans clustering analysis. Web scraping is an essential data collection tools for cases where an API is not readily available.\n",
    "\n",
    "#### Why Selenium\n",
    "Selenium is an automated testing tool for web pages. The reason selenium is being used is because the site that is to be scraped has some of its pages rendered in java script so using the requests library to download the html doesnt't give the relevant data. The selenium library will be used to render the javascript in the browser before the html is downloaded.\n",
    "\n",
    "BeautifoulSoup library will be used to parse the html data into a `pandas.DataFrame` object and the stats file, as well as other files such as the club squads data will be stored as a csv file. A data ditionary was scraped from the site too and stored as a csv file.\n",
    "\n",
    "The key decision to save each html file before parsing them was to increase the interval between requests sent to the sites server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a467ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19724937-38ec-40bc-ab86-c9a48a0f1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(url: str, filename: str) -> None:\n",
    "    \"\"\"Function to download html of Fbref stats page\"\"\"\n",
    "    \n",
    "    with webdriver.Chrome() as driver:\n",
    "        driver.get(url)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\") # simulates a scroll to the end of the webpage\n",
    "        time.sleep(1)\n",
    "        data = driver.page_source\n",
    "    \n",
    "    with open(f\"{filename}.html\", \"w+\", encoding = \"utf-8\") as f:\n",
    "        f.write(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210c32e8-c258-43ce-ab89-fdd411604479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(filename: str, id_: str) -> None:\n",
    "    \"\"\"Function to generate csv files from he downloaded stats html files\"\"\"\n",
    "    \n",
    "    with open(F\"{filename}.html\", encoding = \"utf-8\") as f:\n",
    "        page = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    player_table = soup.find(\"table\", id = id_)\n",
    "    for element in player_table.find_all(\"tr\", class_=\"thead\"):\n",
    "        element.decompose() # removes every tr element with  class \"thead\"\n",
    "    \n",
    "    player_stats = pd.read_html(str(player_table))[0]\n",
    "    # player_stats.columns = player_stats.columns.droplevel()\n",
    "    \n",
    "    player_stats.to_csv(f\"{filename}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b8ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_stats_data() -> None:\n",
    "    urls = [\"https://fbref.com/en/comps/Big5/gca/players/Big-5-European-Leagues-Stats\",\n",
    "            \"https://fbref.com/en/comps/Big5/shooting/players/Big-5-European-Leagues-Stats\",\n",
    "            \"https://fbref.com/en/comps/Big5/passing/players/Big-5-European-Leagues-Stats\",\n",
    "            \"https://fbref.com/en/comps/Big5/defense/players/Big-5-European-Leagues-Stats\",\n",
    "            \"https://fbref.com/en/comps/Big5/keepers/players/Big-5-European-Leagues-Stats\"\n",
    "           ]\n",
    "    # contains the file names and html id for each\n",
    "    stats = [(\"gca.\", \"stats_gca\"), (\"shooting\", \"stats_shooting\"), (\"passing\", \"stats_passing\"), \n",
    "             (\"defense\", \"stats_defense\"), (\"goalkeeping\", \"stats_keeper\")]\n",
    "    \n",
    "    for url, stat in zip(urls, stats):\n",
    "        filename = stat[0]\n",
    "        id_ = stat[1]\n",
    "        get_stats(url, filename)\n",
    "        parse_html(filename, id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4338e6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_stats_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3b0ffe-bc39-4de6-8ad7-772a05e2093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(filename: str) -> None:\n",
    "    \n",
    "    with open(f\"{filename}.html\", encoding = \"utf-8\") as f:\n",
    "        page = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    data_dict = soup.find_all(\"th\", class_ = \"poptip\")\n",
    "    \n",
    "    column_names = []\n",
    "    values = []\n",
    "    details = []\n",
    "\n",
    "    for stat in data_dict:\n",
    "        column_name = stat.text\n",
    "        value = stat[\"aria-label\"]\n",
    "        try:\n",
    "            detail = stat[\"data-tip\"].replace(\"<br>\", \" \").replace(\"<strong>\", \"\").replace(\"</strong>\", \".\")\n",
    "        except: \n",
    "            detail = \"\"\n",
    "    \n",
    "        column_names.append(column_name)\n",
    "        values.append(value)\n",
    "        details.append(detail)\n",
    "    \n",
    "    df = pd.DataFrame({\"column_name\" : column_names, \"value\" : values, \"details\" : details})\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(f\"{filename}_dict.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32f9bcbf-ce20-4a61-bcc0-608042ec6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_list = [\"gca\", \"shooting\", \"passing\",\n",
    "             \"defense\",\"goalkeeping\"]\n",
    "for stats in stats_list:\n",
    "    get_data_dict(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd971303-247b-4cb2-be0f-69e6cf7ff08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laliga_squads() -> None:\n",
    "    \"\"\"Function to scrape La Liga club page, extract the squad list and convert the result to a csv file\"\"\"\n",
    "    \n",
    "    content = requests.get(\"https://www.laliga.com/en-GB/laliga-santander/clubs\")\n",
    "    soup = BeautifulSoup(content.text, \"html.parser\")\n",
    "\n",
    "    # extract link to squad page for each club\n",
    "    links = soup.find_all(\"div\", class_=\"styled__ItemContainer-fyva03-1\")\n",
    "    club_links = []\n",
    "    for link in links:\n",
    "        link = link.find(\"a\")[\"href\"]\n",
    "        club_link = f\"https://www.laliga.com{link}/squad\"\n",
    "        club_links.append(club_link)\n",
    "\n",
    "    # create a pandas dataframe of club squad data    \n",
    "    laliga_squads = []\n",
    "    for club_link in club_links:\n",
    "        club_name = club_link.replace(\"https://www.laliga.com/en-GB/clubs/\", \"\").replace(\"/squad\", \"\").replace(\"-\", \" \").title()\n",
    "        content = requests.get(club_link)\n",
    "        data = content.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        names = []\n",
    "        for element in soup.find_all(\"div\", class_=\"styled__PlayerName-sc-148d0nz-4 bzHSBG\"):\n",
    "            name = element.find_all(\"p\")\n",
    "            names.append(name)\n",
    "\n",
    "        player_names = []\n",
    "        player_positions = []\n",
    "        \n",
    "\n",
    "        for name in names:\n",
    "            player_name = name[0].text\n",
    "            player_position = name[1].text\n",
    "            player_names.append(player_name)\n",
    "            player_positions.append(player_position)\n",
    "        df = pd.DataFrame({\"player_name\" : player_names, \"position\" : player_positions, \"club\" : club_name})\n",
    "        laliga_squads.append(df)\n",
    "        time.sleep(1)\n",
    "        \n",
    "    combined = pd.concat(laliga_squads)\n",
    "    combined.to_csv(\"laliga_squads.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca0ad28b-f659-4acd-a8bf-6d042ac6261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ligue1_squads() -> None:\n",
    "    \n",
    "    \"\"\"Function to scrape Ligue1 club page, extract the squad list and convert the result to a csv file\"\"\"\n",
    "    \n",
    "    ligue1_clublist = requests.get(\"https://www.ligue1.com/clubs/List\")\n",
    "    ligue1_data = ligue1_clublist.text\n",
    "\n",
    "    ligue1_soup = BeautifulSoup(ligue1_data, \"html.parser\")\n",
    "\n",
    "    club_list = ligue1_soup.find(\"div\", class_=\"ClubListPage-list\")\n",
    "\n",
    "    # extract link for club squads page\n",
    "    ligue1_club_links = []\n",
    "    for element in club_list.find_all(\"a\", class_=\"ClubListPage-link\"):\n",
    "        element = element[\"href\"]\n",
    "        link = f\"https://www.ligue1.com{element}\"\n",
    "        link = link[:28] + \"/squad\" + link[28:]\n",
    "        ligue1_club_links.append(link)\n",
    "        \n",
    "    # create a pandas dataframe of club squad data\n",
    "    ligue1_squads = []\n",
    "    for link in ligue1_club_links:\n",
    "        club_name = link.replace(\"https://www.ligue1.com/clubs/squad?id=\", \"\").replace(\"-\", \" \").title()\n",
    "        content = requests.get(link)\n",
    "        data = content.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        club_squad = soup.find_all(\"div\", class_ = \"SquadTeamTable-flip-card\")\n",
    "        \n",
    "        player_names = []\n",
    "        positions = []\n",
    "        \n",
    "        for player in club_squad:\n",
    "            player_name = player.find(\"span\", class_=\"SquadTeamTable-playerName\").text\n",
    "    \n",
    "            position = player.find(\"span\", class_=\"SquadTeamTable-position\").text\n",
    "    \n",
    "            player_names.append(player_name)\n",
    "            positions.append(position)\n",
    "        \n",
    "        df = pd.DataFrame({\"player_name\" : player_names, \"position\" : positions, \"club\" : club_name})\n",
    "        ligue1_squads.append(df)\n",
    "        time.sleep(1)\n",
    "        \n",
    "    combined = pd.concat(ligue1_squads)\n",
    "    combined.to_csv(\"ligue1_squads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef404b3f-5e7c-4306-b7ba-345a87c8e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundesliga_squad() -> None:\n",
    "    \"\"\"Function to scrape bundesliga player page and convert the result to a csv\"\"\"\n",
    "    \n",
    "    bund_content = requests.get(\"https://www.bundesliga.com/en/bundesliga/player\")\n",
    "    bund_soup =  BeautifulSoup(bund_content.text, \"html.parser\")\n",
    "    clubs = []\n",
    "    \n",
    "    # create a pandas dataframe with the players position for each club\n",
    "    for club in bund_soup.find_all(\"mat-expansion-panel\"):\n",
    "        club_name = club.find(\"span\").find(\"h2\").text\n",
    "        players = club.find_all(\"div\", class_=\"row\")\n",
    "        \n",
    "        gk = players[0]\n",
    "        gk_position =  gk.find(\"div\", class_ = \"position\").text\n",
    "        goalkeepers = gk.find_all(\"a\")\n",
    "        goalkeepers = [a[\"href\"].replace(\"/en/bundesliga/player/\", \"\").replace(\"-\", \" \").title() for a in goalkeepers]\n",
    "        goalkeepers = pd.DataFrame({\"players\": goalkeepers, \"position\": gk_position, \"club\": club_name})\n",
    "        \n",
    "        df = players[1]\n",
    "        df_position = df.find(\"div\", class_ = \"position\").text\n",
    "        defenders = df.find_all(\"a\")\n",
    "        defenders = [a[\"href\"].replace(\"/en/bundesliga/player/\", \"\").replace(\"-\", \" \").title() for a in defenders]\n",
    "        defenders = pd.DataFrame({\"players\": defenders, \"position\": df_position, \"club\": club_name})\n",
    "        \n",
    "        mf = players[2]\n",
    "        mf_position = mf.find(\"div\", class_ = \"position\").text\n",
    "        midfielders = mf.find_all(\"a\")\n",
    "        midfielders = [a[\"href\"].replace(\"/en/bundesliga/player/\", \"\").replace(\"-\", \" \").title() for a in midfielders]\n",
    "        midfielders = pd.DataFrame({\"players\": midfielders, \"position\": mf_position, \"club\": club_name})\n",
    "        \n",
    "        att = players[3]\n",
    "        att_position = att.find(\"div\", class_ = \"position\").text\n",
    "        attackers = att.find_all(\"a\")\n",
    "        attackers = [a[\"href\"].replace(\"/en/bundesliga/player/\", \"\").replace(\"-\", \" \").title() for a in attackers]\n",
    "        attackers = pd.DataFrame({\"players\": attackers, \"position\": att_position, \"club\": club_name})\n",
    "        \n",
    "        club_squad = pd.concat([goalkeepers, defenders, midfielders, attackers])\n",
    "        clubs.append(club_squad)\n",
    "        \n",
    "    bundesliga_squad = pd.concat(clubs)\n",
    "    bundesliga_squad.to_csv(\"bundesliga_squads.csv\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c74caf1-5975-4094-a115-2157ea917430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pl_squads() -> None:\n",
    "    \"\"\"Function to scrape Premier League club page, extract the squad list and convert the result to a csv file\"\"\"\n",
    "    \n",
    "    pl = requests.get(\"https://www.premierleague.com/clubs\")\n",
    "    pl_soup = BeautifulSoup(pl.text)\n",
    "\n",
    "    # extract link to go to the squad page of each premier league club\n",
    "    pl_links = [link[\"href\"] for link in pl_soup.find_all(\"a\", class_ = \"indexItem\")]\n",
    "    pl_links = [f\"https://www.premierleague.com{link}\" for link in pl_links]\n",
    "    pl_links = [link.replace(\"overview\", \"squad\") for link in pl_links]\n",
    "    \n",
    "    # create a pandas dataframe of club squad data\n",
    "    pl_squads = []\n",
    "    for link in pl_links:\n",
    "        club_name = re.match(\"https://www.premierleague.com/clubs/[\\d]+/(\\w+-?\\w+.+)/squad\", link).groups()[0]\n",
    "        content = requests.get(link)\n",
    "        soup = BeautifulSoup(content.text)\n",
    "        \n",
    "        player_names = [\n",
    "            name[\"href\"].split(\"/\")[3].replace(\"-\", \" \") for name in soup.find_all(\"a\", class_= \"stats-card__wrapper\")\n",
    "        ]\n",
    "        positions = [position.text for position in soup.find_all(\"div\", class_= \"stats-card__player-position\")]\n",
    "        \n",
    "        df = pd.DataFrame({\"player_name\" : player_names, \"position\" : positions, \"club\" : club_name})\n",
    "        pl_squads.append(df)\n",
    "        time.sleep(1)\n",
    "        \n",
    "    combined = pd.concat(pl_squads)\n",
    "    combined.to_csv(\"pl_squads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71df1156-8b3f-4631-80e5-c53b73221f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_serie_a_squad() -> None:\n",
    "    \"\"\"Function to scrape Serie A club page, extract the squad list and convert the result to a csv file\"\"\"\n",
    "    \n",
    "    url = \"https://www.legaseriea.it/it/serie-a/squadre\"\n",
    "    content = requests.get(url)\n",
    "    soup = BeautifulSoup(content.text)\n",
    "\n",
    "    # extract link to squad page for each club\n",
    "    seriea_clublinks = [a[\"href\"] for a  in soup.find_all(\"a\")[0:20]]\n",
    "    seriea_clublinks = [f\"https://www.legaseriea.it{link}/squadra\" for link in seriea_clublinks]\n",
    "\n",
    "\n",
    "    # using selenium cause most of the page is rendered with java script\n",
    "    serie_a_squads = []\n",
    "    for link in seriea_clublinks:\n",
    "        with webdriver.Chrome() as driver:\n",
    "            driver.get(link)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\") \n",
    "            driver.maximize_window()\n",
    "            time.sleep(3)\n",
    "            data = driver.page_source\n",
    "            \n",
    "        club_name = link.replace(\"https://www.legaseriea.it/it/team/\", \"\").replace(\"/squadra\", \"\").title()\n",
    "        soup = BeautifulSoup(data)\n",
    "        player_data = soup.find_all(\"div\", class_=\"hm-card-body\")\n",
    "        player_names = [player.find_all(\"p\")[0].text for player in player_data]\n",
    "        positions = [player.find_all(\"p\")[1].text for player in player_data]\n",
    "        df = pd.DataFrame({\"player_name\" : player_names, \"position\" : positions, \"club\" : club_name})\n",
    "        df[\"position\"] = (df[\"position\"].str.replace(\"Portiere\", \"Goalkeeper\", regex=True)\n",
    "         .str.replace(\"Portiere\", \"Goalkeeper\", regex=True)\n",
    "         .str.replace(\"Difensore\", \"Defender\", regex=True)\n",
    "         .str.replace(\"Centrocampista\", \"Midfielder\", regex=True)\n",
    "         .str.replace(\"Attaccante\", \"Forward\", regex=True)\n",
    "        )\n",
    "        serie_a_squads.append(df)\n",
    "        time.sleep(1)\n",
    "        \n",
    "    combined = pd.concat(serie_a_squads)\n",
    "    combined.to_csv(\"serie_a_squads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bb5522b-caf5-4207-8539-f81efb72b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_squads() -> None:\n",
    "    get_laliga_squads()\n",
    "    get_ligue1_squads()\n",
    "    get_bundesliga_squad()\n",
    "    get_pl_squads()\n",
    "    get_serie_a_squad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "806d9e0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_squads()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
